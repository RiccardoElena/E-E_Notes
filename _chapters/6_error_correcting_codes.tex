\chapter{Codici correttori d'errori}

Fino a ora per quanto riguarda la codifica di canale abbiamo dato una descrizione astratta di alcune proprietà, senza entrare nel dettaglio di come costruire effettivamente dei codici che permettano di correggere gli errori introdotti dal canale.
Andiamo a vedere ora come costruire tali codici.

Da ora in poi si assumerà che qualsiasi codifica sia su alfabeto binario.

\begin{definition}{Distanza di Hamming in \(\set{0,1}^n\)}
  La \keyword{distanza di Hamming} tra due stringhe binarie \(x,y \in \set{0,1}^n\) è il numero di posizioni in cui le due stringhe differiscono, ovvero
  \[d(x,y) = \sum_{i=1}^n \abs{x_i - y_i}\]
  dove \(x=x_1\ldots x_n\) e \(y=y_1\ldots y_n\).
\end{definition}

La distanza di Hamming è una vera e propria metrica, infatti \(\forall x,y,z \in \set{0,1}^n\)
\begin{enumerate}
  \item \(d(x,y) = d(y,x)\)
  \item \(d(x,y) \geq 0\) e \(d(x,y) = 0 \iff x=y\)
  \item \(d(x,z) \leq d(x,y) + d(y,z)\) (disuguaglianza triangolare)
\end{enumerate}

\begin{proposition}{}
  Su un canale binario simmetrico con probabilità di errore \(q\) usato senza feedback, si ha che
  \[\forall x,y \in \set{0,1}^n, p(y|x) = q^{d(x,y)} {(1-q)}^{n-d(x,y)}\]
\end{proposition}
\begin{proof}
  Basta ricordare che \(p(y|x) = \prod_{i=1}^n p(y_i|x_i)\) e che \(p(y_i|x_i) = q\) se \(y_i \neq x_i\) e \(1-q\) altrimenti.
  Ovviamente il numero di posizioni in cui \(y_i \neq x_i\) è proprio \(d(x,y)\) e quello in cui \(y_i = x_i\) è \(n-d(x,y)\).
\end{proof}

\section{Scelta della decodifica}

Dato un canale con caratteristiche note, e una funzione di codifica \(f: \mathcal{W} \to \set{0,1}^n\), idealmente dovremmo scegliere \(g\) che minimizzi gli errori.
Dato \(y \in \set{0,1}^n\), avrebbe senso porre \(g(y) = f^{-1}(x)\), dove \(x\in\set{0,1}^n\) massimizza \(p(x|y)\), ma ciò richiederebbe di conoscere la distribuzione di \(W\), visto che \(p(x|y) = \frac{p(x,y)}{p(y)} = \frac{p(x)p(y|x)}{p(y)}\), ma in generale il ricevente conosce solo le proprietà del canale (ovvero \(p(y|x)\)).

Una soluzione \qi{pratica} è quella di scegliere \(g(y) = f^-1(x)\) dove \(x\) massimizza \(p(y|x)\).
Un altra possibile scelta consiste nel minimizzare \(d(x,y)\).

\begin{proposition}{}
  Su un canale binario simmetrico usato senza feedback con probabilità di errore \(q \leq \frac{1}{2}\) le due strategie coincidono, ovvero
  \[p(y|x) = \max_{x' \in f(\mathcal{W})} p(y|x') \iff d(x,y) = \min_{x' \in f(\mathcal{W})} d(x',y)\]
\end{proposition}

\begin{proof}{}
  Siano \(x,x' \in f(\mathcal{W})\), \(j=d(x,y), j'=d(x',y)\). Allora
  \[\frac{p(y|x)}{p(y|x')} = \frac{q^j{(1-q)}^{n-j}}{q^{j'}{(1-q)}^{n-j'}} = {\left(\frac{1-q}{q}\right)}^{j'-j}\]
  Essendo \(q < \frac{1}{2}\), allora \(\frac{1-q}{q} > 1\) e dunque \(\frac{1-q}{q} > 1\).

  E dunque \[\frac{p(y|x)}{p(y|x')} > 1 \iff j' - j > 0 \iff j < j'\]
\end{proof}

\begin{observation}{}
  A seconda della seconda del codice \(Z = f(\mathcal{W})\) utilizzato, potrebbero esserci più \(x \in \mathcal{Z}\) aventi distanza minima da \(y \in \mathcal{Y}\).
  \begin{example}{}
    Sia \(Z = \set{0^8,00111100,11 0^5 1,0^4 1^3 0,101^3 001,00110110,11001011}\) e \(y = 11001001\). Allora chiaramente \(y \not\in \mathcal{Z}, (\forall x \in \mathcal{Z}, d(x,y)\neq 0)\), ma sia \(x=110^5 1\) che \(x' = 11001011\) danno \(d(x,y)=d(x',y) = 1\).
  \end{example}

  Di conseguenza è necessario aggiungere criteri ulteriori per risolvere le ambiguità.
\end{observation}

\begin{definition}{Minima distanza di un codice}
  Dato un codice uniforme \(Z\subseteq \set{0,1}^n\), chiamiamo \keyword{minima distanza di \(Z\)} la  quantità
  \[\delta_Z = \min_{\substack{x,x' \in Z \\ x\neq x'}} d(x,x')\]
\end{definition}

\begin{example}{}
  Prendendo \(Z\) dall'esempio precedente, si ha che \(\delta_Z = 3\).
\end{example}

\begin{definition}{Intorno}
  Dato \(x \in \set{0,1}^n\) e \(r \geq 0\), definiamo l'\keyword{intorno di raggio \(r\) centrato in \(x\)} come
  \[N_r(x) = \set{y \in \set{0,1}^n : d(x,y) \leq r}\]
\end{definition}

Vediamo dunque un risultato in cui queste definizioni vanno insieme.

\begin{lemma}{}
  Dato \(Z\subseteq\set{0,1}^n\) con \(\delta_Z \geq 2r+1\) per un certo \(r\geq 0\), allora
  \[\forall x,x' \in Z, x\neq x' \implies N_r(x) \cap N_r(x') = \emptyset\]
\end{lemma}

\begin{proof}
  Siano \(z,z' \in Z\) e assumiamo che \(N_r(z) \cap N_r(z') \neq \emptyset\) e sia dunque \(u \in \N_r(z) \cap N_r(z')\).
  Allora dalla disuguaglianza triangolare si ha che
  \[d(z,z') \leq d(z,u) + d(u,z') \leq r + r = 2r\]
  Dunque abbiamo trovato due parole \(z,z' \in Z\) con distanza minore della distanza minima del codice, dunque \(z=z'\).
\end{proof}

\begin{theorem}{}
  Sia \(Z = f(\mathcal{W})\) codice binario uniforme con \(\delta_Z \geq 2r+1\) per un certo \(r\geq 0\), trasmesso su un BSC con probabilità di errore \(q\leq \frac{1}{2}\) usato senza feedback.
  Allora usando la strategia della minima distanza, ogni output con al più \(r\) errori è decodificato correttamente.
\end{theorem}

\begin{proof}{}
  Sia \(x \in f(\mathcal{W})\) e sia \(y \in N_r(x)\) l'output ricevuto.
  Dal lemma precedente, \(\forall x' \in Z\setminus\set{x}, N_r(x) \cap N_r(x') = \emptyset\) e dunque \(y \not\in N_r(x')\) da cui \(d(x',y) > r\).
  Dunque \(d(x,y) < d(x',y)\) e quindi la decodifica tramite minima distanza imporrà \(g(y) = f^{-1}(x)\).
\end{proof}




